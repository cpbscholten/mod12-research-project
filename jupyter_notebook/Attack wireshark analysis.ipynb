{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wireshark Honeypot attack analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import sys\n",
    "import pyasn\n",
    "import csv\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pycountry\n",
    "import geopandas as gpd\n",
    "import geoplot as gplt\n",
    "import geoplot.crs as gcrs\n",
    "from shapely.geometry import Point\n",
    "from scapy.all import *\n",
    "# install with: pip3 install maxminddb-geolite2 --user\n",
    "from geolite2 import geolite2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# location of complete wireshark capture\n",
    "\n",
    "file_name = \"tcpdump/honeypot_traffic_filtered.pcapng\"\n",
    "# location file filtered for payload 1\n",
    "file_name_cve_2018_14847_tcp_streams = \"tcpdump/honeypot_traffic_cve_2018_14847.pcapng\"\n",
    "file_name_dns_redirection = \"tcpdump/honeypot_traffic_dns_redirection.pcapng\"\n",
    "file_name_mirai = \"tcpdump/honeypot_traffic_rewrite_mirai.pcap\"\n",
    "# ip address of the server\n",
    "server_ip = \"0.0.0.0\" # change\n",
    "real_server_ip = \"0.0.0.0\" # update to real ip\n",
    "\n",
    "# time stamp period_\n",
    "timestamp_period = 43200 # 12 hours\n",
    "\n",
    "# ASN database location\n",
    "# see pyasn documentation: https://github.com/Yelp/pyasn\n",
    "asndb_file = \"utils/ipasndb-2019-0529.dat\"\n",
    "asndb = pyasn.pyasn(asndb_file)\n",
    "\n",
    "# ASN List file with names\n",
    "aslist_csv = \"utils/aslist.csv\"\n",
    "aslist_dict = {}\n",
    "import csv\n",
    "with open(aslist_csv, 'rt') as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in csvreader:\n",
    "        aslist_dict[row[0]] = row[1]\n",
    "\n",
    "# ip info reader\n",
    "ip_reader = geolite2.reader()\n",
    "\n",
    "# countries shapefile\n",
    "countries_shapefile = 'utils/110m_cultural/ne_110m_admin_0_countries.shp'\n",
    "countries_gdf = gpd.read_file(countries_shapefile)[['ADMIN', 'ADM0_A3', 'geometry']]\n",
    "countries_gdf.columns = ['country', 'country_code', 'geometry']\n",
    "\n",
    "ip_reader.get(real_server_ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets count the number of packets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "general_count = 0\n",
    "general_data_count = 0\n",
    "if not os.path.isfile(file_name):\n",
    "        print('\"{}\" does not exist'.format(file_name), file=sys.stderr)\n",
    "        sys.exit(-1)\n",
    "for (pkt_data, pkt_metadata,) in RawPcapReader(file_name):\n",
    "    general_count += 1\n",
    "    general_data_count += len(pkt_data)\n",
    "print('{} contains {} packets'.format(file_name, general_count))\n",
    "print('{} has a size of {} bytes'.format(file_name, general_data_count))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of unique ip's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "general_ip_data = {}\n",
    "general_src_port_count = {}\n",
    "general_dest_port_count = {}\n",
    "general_data_time_continent_count = {}\n",
    "count=0\n",
    "for (pkt_data, pkt_metadata,) in RawPcapReader(file_name):   \n",
    "    ether_pkt = Ether(pkt_data)\n",
    "    ip_pkt = ether_pkt[IP]\n",
    "    \n",
    "    # skip non tcp packets\n",
    "    if not ether_pkt.haslayer(TCP):\n",
    "        print(\"skipped a packet\")\n",
    "        continue\n",
    "        \n",
    "    tcp_pkt = ip_pkt[TCP]\n",
    "    \n",
    "    # skip packets in the direction server -> client\n",
    "    src = str(ip_pkt.src)\n",
    "    if src == server_ip:\n",
    "        continue      \n",
    "        \n",
    "    count += 1\n",
    "    \n",
    "    # packet size\n",
    "    pkt_len = len(pkt_data)\n",
    "    \n",
    "    if src not in general_ip_data:\n",
    "        asn, bgp_range = asndb.lookup(src)\n",
    "        asn = \"AS\" + str(asn)\n",
    "        asn_owner = aslist_dict[asn] if asn in aslist_dict else \"\"\n",
    "        ip_dict = ip_reader.get(src)\n",
    "        if ip_dict:\n",
    "            country_code = ip_dict['country']['iso_code'] if 'country' in ip_dict.keys() else \"N/A\"\n",
    "            country = ip_dict['country']['names']['en'] if 'country' in ip_dict.keys() else \"N/A\"\n",
    "            continent = ip_dict['continent']['code'] if 'continent' in ip_dict.keys() else \"N/A\"\n",
    "            city = ip_dict['city']['names']['en'] if 'city' in ip_dict.keys() else \"N/A\"\n",
    "            lat = ip_dict['location']['latitude'] if 'location'in ip_dict.keys() else \"N/A\"\n",
    "            long = ip_dict['location']['longitude'] if \"location\" in ip_dict.keys() else \"N/A\"\n",
    "            lat_long = (lat, long)\n",
    "        else:\n",
    "            country_code = \"N/A\"\n",
    "            country = \"N/A\"\n",
    "            continent = \"N/A\"\n",
    "            city = \"N/A\"\n",
    "            lat_long = \"\"\n",
    "        general_ip_data[src] = [1, pkt_len, asn, bgp_range, asn_owner, country_code, country, continent, city, lat_long]\n",
    "    else:\n",
    "        general_ip_data[src][0] += 1\n",
    "        general_ip_data[src][1] += pkt_len\n",
    "        \n",
    "    # generate timestamp\n",
    "    pkt_timestamp = (pkt_metadata.tshigh << 32) | pkt_metadata.tslow\n",
    "    pkt_timestamp_resol = pkt_metadata.tsresol # microseconds (1.000.000) or nanoseconds (1.000.000.000)\n",
    "    pkt_sec = pkt_timestamp // pkt_timestamp_resol\n",
    "    \n",
    "    # set timestamp seconds of first packet and last packet\n",
    "    if count == 1:\n",
    "        first_pkt_sec = pkt_timestamp // pkt_timestamp_resol\n",
    "    last_pkt_sec = pkt_sec\n",
    "    \n",
    "    # create 10 minute timeframe timestamp and set time at middle of time frame\n",
    "    time_period_middle_sec = pkt_sec - (pkt_sec - first_pkt_sec) % timestamp_period + (timestamp_period // 2)\n",
    "        \n",
    "    continent = general_ip_data[src][7]\n",
    "    # generate continent time data dictionary\n",
    "    if continent not in general_data_time_continent_count:\n",
    "        general_data_time_continent_count[continent] = {str(time_period_middle_sec): pkt_len}\n",
    "    elif str(time_period_middle_sec) not in general_data_time_continent_count[continent]:\n",
    "        general_data_time_continent_count[continent][str(time_period_middle_sec)] = pkt_len\n",
    "    else:\n",
    "        general_data_time_continent_count[continent][str(time_period_middle_sec)] += pkt_len \n",
    "        \n",
    "    # source port count\n",
    "    src_port = str(tcp_pkt.sport)\n",
    "    if src_port not in general_src_port_count:\n",
    "        general_src_port_count[src_port] = 1\n",
    "    else:\n",
    "        general_src_port_count[src_port] += 1\n",
    "    \n",
    "    # destination port count\n",
    "    dest_port = str(tcp_pkt.dport)\n",
    "    if dest_port not in general_dest_port_count:\n",
    "        general_dest_port_count[dest_port] = 1\n",
    "    else:\n",
    "        general_dest_port_count[dest_port] += 1\n",
    "    \n",
    "print(\"A total of {} incoming packets were analyzed\".format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create normal dict for data_time_continent that can be converted to dataframe\n",
    "number_time_periods = (last_pkt_sec - first_pkt_sec) // timestamp_period\n",
    "general_data_time_continent_table = {}\n",
    "general_data_time_continent_table_columns = []\n",
    "# initialize lisr for each continent\n",
    "for continent in general_data_time_continent_count.keys():\n",
    "    general_data_time_continent_table[continent] = []\n",
    "# initialize timestamp and sizes for continents\n",
    "for i in range(0, number_time_periods):\n",
    "    timestamp = first_pkt_sec + i * timestamp_period + timestamp_period // 2\n",
    "    # convert timestamp in seconds to pandas timestamp in UTC\n",
    "    unix_timestamp = pd.Timestamp(timestamp, unit='s') \n",
    "    general_data_time_continent_table_columns.append(unix_timestamp)\n",
    "    # add packet traffic for each continent for this timestamp\n",
    "    for continent in general_data_time_continent_count.keys():\n",
    "        if str(timestamp) in general_data_time_continent_count[continent]:\n",
    "            pkt_len = general_data_time_continent_count[continent][str(timestamp)]\n",
    "        else:\n",
    "            pkt_len = 0\n",
    "        general_data_time_continent_table[continent].append(pkt_len)\n",
    "\n",
    "# create dataframe \n",
    "general_data_time_continent_df = pd.DataFrame.from_dict(general_data_time_continent_table, orient=\"index\", columns=general_data_time_continent_table_columns)\n",
    "general_data_time_continent_df = general_data_time_continent_df.T\n",
    "general_data_time_continent_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "general_data_time_continent_plot = general_data_time_continent_df.plot.area(figsize=(5, 3.3), cmap='YlGnBu')\n",
    "general_data_time_continent_plot.set_ylabel('bytes')\n",
    "general_data_time_continent_plot.set_xlabel('time')\n",
    "plt.savefig(\"images/data_time_continent.pdf\", bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IP-addresses it belong to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "general_ip_res = []\n",
    "for ip, info in general_ip_data.items():\n",
    "    res = [ip]\n",
    "    res.extend(info)\n",
    "    general_ip_res.append(res)\n",
    "print(\"There are {} unique ip addresses.\".format(len(general_ip_data)))\n",
    "general_ip_df = pd.DataFrame(general_ip_res, columns=['ip_address', 'no_of_packets', \"no_of_bytes\", \"asn\", \"bgp_range\", \"asn_owner\", \"country_code\", \"country\", \"continent\", \"city\", \"location\"])\n",
    "# general_ip_df.set_index('ip_address', inplace=True)\n",
    "general_ip_df.sort_values(by=['no_of_packets'], ascending=[False], inplace=True)\n",
    "# reset index and start at 1\n",
    "general_ip_df.index = np.arange(1, len(general_ip_df) + 1)\n",
    "# print top 20\n",
    "general_ip_df[:20]\n",
    "\n",
    "half_packet_ip_count = 0\n",
    "half_packet_count = 0\n",
    "while half_packet_count < count // 2:\n",
    "    half_packet_ip_count += 1\n",
    "    half_packet_count += general_ip_df.at[half_packet_ip_count,'no_of_packets']\n",
    "    \n",
    "print(\"Half of the packets are send by the top {} IP addresses\".format(half_packet_ip_count))\n",
    "print(\"This is {}% of the total IP\".format(100/len(general_ip_data)*half_packet_ip_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_ip_df[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of packets per ip plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ip_packet_plot = general_ip_df.plot(kind='line', y='no_of_packets', ylim=(0.5,10000), logy=True, logx=False, legend=False)\n",
    "ip_packet_plot.set_xlabel('Top # IP address')\n",
    "ip_packet_plot.set_ylabel('Number of packets')\n",
    "plt.savefig(\"images/ip_packet_plot.pdf\", bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "general_ip_packet_list = general_ip_df['no_of_packets'].values.tolist()\n",
    "general_ip_packet_series = pd.Series(general_ip_packet_list)\n",
    "general_ip_packet_np_arr = np.array(general_ip_packet_list)\n",
    "general_ip_packet_np_arr_sorted = np.sort(general_ip_packet_np_arr)[::-1]\n",
    "general_ip_packet_np_arr_sorted_cumsum = np.cumsum(general_ip_packet_np_arr_sorted)\n",
    "max_val = general_ip_packet_np_arr_sorted_cumsum[len(general_ip_packet_np_arr_sorted_cumsum)-1]\n",
    "arr = [0]\n",
    "arr.extend([x / max_val for x in general_ip_packet_np_arr_sorted_cumsum])\n",
    "# print(arr)\n",
    "general_ip_packet_np_ar_perc = np.array(arr)\n",
    "\n",
    "# plot the sorted data:\n",
    "fig = plt.figure(figsize=(4,2.2))\n",
    "ax2 = fig.add_subplot()\n",
    "ax2.plot(general_ip_packet_np_ar_perc)\n",
    "ax2.set_xlabel('# IP addresses')\n",
    "ax2.set_ylabel('percentage of packets')\n",
    "ax2.set_xscale('symlog')\n",
    "ax2.set_ylim(bottom=0)\n",
    "ax2.set_xlim(xmin=0)\n",
    "plt.savefig(\"images/ip_packet_cdf.pdf\", bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# generate dict of location and bytes:\n",
    "general_loc_dict = {}\n",
    "for ip, data in general_ip_data.items():\n",
    "    no_packets = data[0]\n",
    "    loc = data[9]\n",
    "    if not loc:\n",
    "        continue\n",
    "    lat, long = loc\n",
    "    if lat == \"N/A\" or long == \"N/A\":\n",
    "        continue\n",
    "    if str(loc) in general_loc_dict:\n",
    "        old_no_packets, _, _ = general_loc_dict[str(loc)]\n",
    "        general_loc_dict[str(loc)] = (old_no_packets + no_packets, lat, long)\n",
    "    else:\n",
    "        general_loc_dict[str(loc)] = (no_packets, lat, long)\n",
    "\n",
    "# filter locations with less than 10 packets\n",
    "keys_to_pop = []\n",
    "for loc, data in general_loc_dict.items():\n",
    "    no_packets, lat, long = data\n",
    "    if no_packets < 10:\n",
    "        keys_to_pop.append(loc)\n",
    "for key in keys_to_pop:\n",
    "    general_loc_dict.pop(key, None)\n",
    "        \n",
    "general_loc_list = general_loc_dict.values()\n",
    "general_loc_df = pd.DataFrame(general_loc_list, columns=['packets', 'lat', 'long'])\n",
    "general_gpd_loc_df = gpd.GeoDataFrame(general_loc_df, geometry=[Point(x, y) for x, y in zip(general_loc_df.long, general_loc_df.lat)])\n",
    "general_gpd_loc_df.sort_values(by=['packets'], ascending=[False], inplace=True)\n",
    "general_gpd_loc_df\n",
    "\n",
    "world_map_df = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "world_map = gplt.polyplot(world_map_df, \n",
    "                          figsize=(6, 3.4), \n",
    "                          zorder=-1, # place the borders below the data\n",
    "                          linewidth=0.5 # make borders thinner\n",
    "                         )\n",
    "                          \n",
    "gplt.pointplot(general_gpd_loc_df, \n",
    "               scale='packets',\n",
    "               limits=(1,100), # smallest circle is 100 times smaller than biggest\n",
    "               hue='packets',\n",
    "               cmap='Blues', # more blue more data\n",
    "               k=6, # limit ourselves to seven sizes\n",
    "               linewidth=0.5,\n",
    "               edgecolor='black',\n",
    "               legend=True,\n",
    "               legend_var='scale', # based on circle size\n",
    "               legend_values=[100000, 50000, 10000, 5000, 1000, 100],\n",
    "               legend_labels=['100.000', '50.000', '10.000', '5.000', '1.000', '100'],\n",
    "               legend_kwargs={'frameon': False, 'loc': 'lower right'},  # ...on the lower right!\n",
    "               ax=world_map\n",
    "              )\n",
    "world_map.axes.get_xaxis().set_visible(False)\n",
    "world_map.axes.get_yaxis().set_visible(False)\n",
    "plt.savefig(\"images/map_attack_locations.pdf\", bbox_inches='tight', pad_inches=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traffic per country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "general_country_df = general_ip_df.groupby(['country_code', 'country']).agg({'no_of_packets': np.sum, 'no_of_bytes': np.sum, 'ip_address': 'nunique', 'asn': 'nunique'})\n",
    "general_country_df.sort_values(by=['no_of_packets'], ascending=[False], inplace=True)\n",
    "general_country_df.reset_index(level=0, inplace=True)\n",
    "\n",
    "# print top 20\n",
    "print(\"There are {} unique countries\".format(len(general_country_df)))\n",
    "general_country_df[:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traffic per country map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# convert two letter country code to three letters\n",
    "countries = {}\n",
    "for country in pycountry.countries:\n",
    "    countries[country.alpha_2] = country.alpha_3\n",
    "    \n",
    "general_country_df_copy = general_country_df.copy()    \n",
    "for index, row in general_country_df_copy.iterrows():\n",
    "    c_code = row['country_code']\n",
    "    general_country_df_copy.at[index, 'country_code'] = countries[c_code] if c_code in countries else \"??\"\n",
    "general_country_df_copy\n",
    "\n",
    "countries_packets_gdf = pd.merge(countries_gdf, general_country_df_copy, how='left', on=['country_code'])\n",
    "countries_packets_gdf.drop('asn', 1, inplace=True)\n",
    "countries_packets_gdf.drop('ip_address', 1, inplace=True)\n",
    "countries_packets_gdf.fillna(0, inplace=True)\n",
    "countries_packets_gdf\n",
    "\n",
    "world_plot = gplt.choropleth(countries_packets_gdf, \n",
    "                hue='no_of_packets',  # Display data, passed as a Series\n",
    "                cmap='Blues',\n",
    "                scheme='fisher_jenks',\n",
    "                linewidth=0.5, \n",
    "                k=10,  # Do not bin our counties.\n",
    "                legend=True,\n",
    "                figsize=(8, 4.5),\n",
    "                legend_labels=['0 - 300', '300 - 950', '950 - 2.100', '2.100 - 3.200', '3.200 - 5.800', '5.800 - 7.600', '7.600 - 9.500', '9.500 - 15.200', '15.200 - 77.000', '77.000 - 200.000']\n",
    "               )\n",
    "world_plot.axes.get_xaxis().set_visible(False)\n",
    "world_plot.axes.get_yaxis().set_visible(False)\n",
    "plt.savefig(\"images/map_attack_countries.pdf\", bbox_inches='tight', pad_inches=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traffic per continent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "general_continent_df = general_ip_df.groupby('continent').agg({'no_of_packets': np.sum, 'no_of_bytes': np.sum, 'ip_address': 'count', 'asn': 'nunique'})\n",
    "general_continent_df.sort_values(by=['no_of_packets'], ascending=[False], inplace=True)\n",
    "general_continent_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Port/packet distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "general_dest_port_df = pd.DataFrame(general_dest_port_count.items(), columns=['dest_port', 'no_of_packets'])\n",
    "general_dest_port_df.set_index('dest_port', inplace=True)\n",
    "general_dest_port_df.sort_values(by=['no_of_packets'], ascending=[False], inplace=True)\n",
    "general_dest_port_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dest_port_bar_graph = general_dest_port_df.plot(kind=\"bar\", y='no_of_packets', legend=False, figsize=(4,2.4))\n",
    "dest_port_bar_graph.set_xlabel('Destination port')\n",
    "dest_port_bar_graph.set_ylabel('Number of packets')\n",
    "plt.savefig(\"images/dest_port_packet_graph.pdf\", bbox_inches='tight', pad_inches=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source port distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "general_src_port_df = pd.DataFrame(general_src_port_count.items(), columns=['src_port', 'no_of_packets'])\n",
    "general_src_port_df.set_index('src_port', inplace=True)\n",
    "general_src_port_df.sort_values(by=['no_of_packets'], ascending=[False], inplace=True)\n",
    "# print the top 20\n",
    "print(\"There are {} unique source ports\".format(len(general_src_port_df)))\n",
    "general_src_port_df[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Autonomous Systems these IPs belong to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "general_asn_df = general_ip_df.groupby(['asn', 'asn_owner']).agg({'no_of_packets': np.sum, 'no_of_bytes': np.sum, 'ip_address': 'count', 'bgp_range': 'unique'})\n",
    "general_asn_df.sort_values(by=['no_of_packets'], ascending=[False], inplace=True)\n",
    "# print the top 20\n",
    "print(\"There are {} unique ASN's\".format(len(general_asn_df)))\n",
    "general_asn_df[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CVE-2018-14847\n",
    "Let's filter for packages containing the CVE-2017-14847 payloads "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First take a look at the packets containing the first payload:\n",
    "```\n",
    "{bff0005:1, uff0006:5, uff0007:7, s1: \n",
    "'/////./..//////./..//////./../flash/rw/store/user.dat', \n",
    "Uff0002:[0,8], Uff0001:[2,2]}\n",
    "```\n",
    "\n",
    "this has already been prepared by first filtering for the tcp streams containing the payload: `tshark -r merged_filtered.pcapng -Y \"tcp contains \\\"/////./..//////./..//////./../flash/rw/store/user.dat\\\"\" -T fields -e tcp.stream | sort -un | sed ':a;N;$!ba;s/\\n/ or tcp.stream==/g'`\n",
    "\n",
    "and then that can be applied as a wireshark filter:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for (pkt_data, pkt_metadata,) in RawPcapReader(file_name_cve_2018_14847_tcp_streams):\n",
    "    count+=1\n",
    "print(\"There are {} packets containing payload 1\".format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will discover all unique ip addresses with this payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "cve_14847_ip_data = {}\n",
    "cve_14847_traffic = {}\n",
    "count = 0\n",
    "for (pkt_data, pkt_metadata,) in RawPcapReader(file_name_cve_2018_14847_tcp_streams):\n",
    "    ether_pkt = Ether(pkt_data)\n",
    "    ip_pkt = ether_pkt[IP]\n",
    "    tcp_pkt = ip_pkt[TCP]\n",
    "    \n",
    "    # skip packets in the direction server -> client\n",
    "    src = str(ip_pkt.src)\n",
    "    if src == server_ip:\n",
    "        continue  \n",
    "    \n",
    "    count +=1\n",
    "    \n",
    "    # packet size\n",
    "    pkt_len = len(pkt_data)\n",
    "    \n",
    "    if src not in cve_14847_ip_data:\n",
    "        asn, bgp_range = asndb.lookup(src)\n",
    "        asn = \"AS\" + str(asn)\n",
    "        asn_owner = aslist_dict[asn] if asn in aslist_dict else \"\"\n",
    "        ip_dict = ip_reader.get(src)\n",
    "        if ip_dict:\n",
    "            country_code = ip_dict['country']['iso_code'] if 'country' in ip_dict.keys() else \"N/A\"\n",
    "            country = ip_dict['country']['names']['en'] if 'country' in ip_dict.keys() else \"N/A\"\n",
    "            continent = ip_dict['continent']['code']\n",
    "            city = ip_dict['city']['names']['en'] if 'city' in ip_dict.keys() else \"N/A\"\n",
    "            lat = ip_dict['location']['latitude']\n",
    "            long = ip_dict['location']['longitude']\n",
    "            lat_long = (lat, long)\n",
    "        else:\n",
    "            country_code = \"\"\n",
    "            country = \"\"\n",
    "            continent = \"\"\n",
    "            city = \"\"\n",
    "            lat_long = \"\"\n",
    "        cve_14847_ip_data[src] = [1, pkt_len, asn, bgp_range, asn_owner, country_code, country, continent, city, lat_long]\n",
    "    else:\n",
    "        old_cve_14847_ip_data = cve_14847_ip_data[src]\n",
    "        new_cve_14847_ip_data = deepcopy(old_cve_14847_ip_data)\n",
    "        new_cve_14847_ip_data[0] += 1\n",
    "        new_cve_14847_ip_data[1] += pkt_len\n",
    "        cve_14847_ip_data[src] = new_cve_14847_ip_data\n",
    "        \n",
    "    # generate timestamp\n",
    "    pkt_timestamp = (pkt_metadata.tshigh << 32) | pkt_metadata.tslow\n",
    "    pkt_timestamp_resol = pkt_metadata.tsresol # microseconds (1.000.000) or nanoseconds (1.000.000.000)\n",
    "    pkt_sec = pkt_timestamp // pkt_timestamp_resol\n",
    "    \n",
    "    # set timestamp seconds of first packet and last packet\n",
    "    if count == 1:\n",
    "        first_pkt_sec = pkt_timestamp // pkt_timestamp_resol\n",
    "    last_pkt_sec = pkt_timestamp // pkt_timestamp_resol\n",
    "    \n",
    "    # create 10 minute timeframe timestamp and set time at middle of time frame\n",
    "    time_period_middle_sec = pkt_sec - (pkt_sec - first_pkt_sec) % timestamp_period + timestamp_period // 2\n",
    "        \n",
    "    # generate continent time data dictionary\n",
    "    if str(time_period_middle_sec) not in cve_14847_traffic:\n",
    "        cve_14847_traffic[str(time_period_middle_sec)] = pkt_len\n",
    "    else:\n",
    "        cve_14847_traffic[str(time_period_middle_sec)] += pkt_len        \n",
    "\n",
    "print(\"There are {} unique ip addresses!\".format(len(cve_14847_ip_data)))\n",
    "\n",
    "cve_14847_ip_res = []\n",
    "for ip, info in cve_14847_ip_data.items():\n",
    "    res = [ip]\n",
    "    res.extend(info)\n",
    "    cve_14847_ip_res.append(res)\n",
    "cve_14847_ip_df = pd.DataFrame(cve_14847_ip_res, columns=['ip_address', 'no_of_packets', \"no_of_bytes\", \"asn\", \"bgp_range\", \"asn_owner\", \"country_code\", \"country\", \"continent\", \"city\", \"location\"])\n",
    "cve_14847_ip_df.set_index('ip_address', inplace=True)\n",
    "cve_14847_ip_df.sort_values(by=['no_of_packets'], ascending=[False], inplace=True)\n",
    "# print top 20\n",
    "cve_14847_ip_df[:20]\n",
    "\n",
    "# string = \"\"\n",
    "# for ip, _ in cve_14847_ip_data.items():\n",
    "#     if len(string) != 0:\n",
    "#         string += \" or \"\n",
    "#     string += \"ip.addr == \"\n",
    "#     string += ip\n",
    "# print(\"Wireshark filter:\")\n",
    "# print(string)\n",
    "print(cve_14847_traffic)\n",
    "count = 0\n",
    "for key, value in cve_14847_traffic.items():\n",
    "    count += value\n",
    "print(\"Total number of: {}  pakcets\".format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df:\n",
    "cve_14847_cols = []\n",
    "cve_14847_pckts = []\n",
    "for key, val in cve_14847_traffic.items():\n",
    "    unix_timestamp = pd.Timestamp(int(key), unit='s')\n",
    "    cve_14847_cols.append(unix_timestamp)\n",
    "    cve_14847_pckts.append(val)\n",
    "cve_14847_cols.pop()\n",
    "cve_14847_pckts.pop()\n",
    "cve_14847_traffic_df = pd.DataFrame(cve_14847_pckts, index=cve_14847_cols, columns=[\"CVE-2018-14847 traffic\"])\n",
    "# cve_14847_traffic_df = cve_14847_traffic_df.T\n",
    "cve_14847_traffic_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cve_14847_traffic_plot = cve_14847_traffic_df.plot.line(figsize=(5, 3), legend=False)\n",
    "cve_14847_traffic_plot.set_ylabel('bytes')\n",
    "cve_14847_traffic_plot.set_xlabel('time')\n",
    "plt.savefig(\"images/cve_14847_traffic.pdf\", bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the AS numbers corresponding to these IP's?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cve_14847_asn_df = cve_14847_ip_df.copy()\n",
    "cve_14847_asn_df.reset_index(level=0, inplace=True)\n",
    "cve_14847_asn_df = cve_14847_asn_df.groupby(['asn', 'asn_owner']).agg({'no_of_packets': 'sum', 'no_of_bytes': 'sum', 'ip_address': 'nunique', 'bgp_range': 'unique'})\n",
    "cve_14847_asn_df.sort_values(by=['no_of_packets'], ascending=[False], inplace=True)\n",
    "cve_14847_asn_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNS redirection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dns_red_ip_data = {}\n",
    "dns_red_traffic = {}\n",
    "count = 0\n",
    "for (pkt_data, pkt_metadata,) in RawPcapReader(file_name_dns_redirection):\n",
    "    ether_pkt = Ether(pkt_data)\n",
    "    ip_pkt = ether_pkt[IP]\n",
    "    tcp_pkt = ip_pkt[TCP]\n",
    "    \n",
    "    # skip packets in the direction server -> client\n",
    "    src = str(ip_pkt.src)\n",
    "    if src == server_ip:\n",
    "        continue\n",
    "    \n",
    "    count +=1\n",
    "    \n",
    "    # packet size\n",
    "    pkt_len = len(pkt_data)\n",
    "    \n",
    "    if src not in dns_red_ip_data:\n",
    "        asn, bgp_range = asndb.lookup(src)\n",
    "        asn = \"AS\" + str(asn)\n",
    "        asn_owner = aslist_dict[asn] if asn in aslist_dict else \"\"\n",
    "        ip_dict = ip_reader.get(src)\n",
    "        if ip_dict:\n",
    "            country_code = ip_dict['country']['iso_code'] if 'country' in ip_dict.keys() else \"N/A\"\n",
    "            country = ip_dict['country']['names']['en'] if 'country' in ip_dict.keys() else \"N/A\"\n",
    "            continent = ip_dict['continent']['code']\n",
    "            city = ip_dict['city']['names']['en'] if 'city' in ip_dict.keys() else \"N/A\"\n",
    "            lat = ip_dict['location']['latitude']\n",
    "            long = ip_dict['location']['longitude']\n",
    "            lat_long = (lat, long)\n",
    "        else:\n",
    "            country_code = \"\"\n",
    "            country = \"\"\n",
    "            continent = \"\"\n",
    "            city = \"\"\n",
    "            lat_long = \"\"\n",
    "        dns_red_ip_data[src] = [1, pkt_len, asn, bgp_range, asn_owner, country_code, country, continent, city, lat_long]\n",
    "    else:\n",
    "        old_dns_red_ip_data = dns_red_ip_data[src]\n",
    "        new_dns_red_ip_data = deepcopy(old_dns_red_ip_data)\n",
    "        new_dns_red_ip_data[0] += 1\n",
    "        new_dns_red_ip_data[1] += pkt_len\n",
    "        dns_red_ip_data[src] = new_dns_red_ip_data\n",
    "        \n",
    "    # generate timestamp\n",
    "    pkt_timestamp = (pkt_metadata.tshigh << 32) | pkt_metadata.tslow\n",
    "    pkt_timestamp_resol = pkt_metadata.tsresol # microseconds (1.000.000) or nanoseconds (1.000.000.000)\n",
    "    pkt_sec = pkt_timestamp // pkt_timestamp_resol\n",
    "    \n",
    "    # set timestamp seconds of first packet and last packet\n",
    "    if count == 1:\n",
    "        first_pkt_sec = pkt_timestamp // pkt_timestamp_resol\n",
    "    last_pkt_sec = pkt_timestamp // pkt_timestamp_resol\n",
    "    \n",
    "    # create 10 minute timeframe timestamp and set time at middle of time frame\n",
    "    time_period_middle_sec = pkt_sec - (pkt_sec - first_pkt_sec) % timestamp_period + timestamp_period // 2\n",
    "        \n",
    "    # generate continent time data dictionary\n",
    "    if str(time_period_middle_sec) not in dns_red_traffic:\n",
    "        dns_red_traffic[str(time_period_middle_sec)] = pkt_len\n",
    "    else:\n",
    "        dns_red_traffic[str(time_period_middle_sec)] += pkt_len        \n",
    "\n",
    "print(\"There are {} unique ip addresses!\".format(len(dns_red_ip_data)))\n",
    "\n",
    "dns_red_ip_res = []\n",
    "for ip, info in dns_red_ip_data.items():\n",
    "    res = [ip]\n",
    "    res.extend(info)\n",
    "    dns_red_ip_res.append(res)\n",
    "dns_red_ip_df = pd.DataFrame(dns_red_ip_res, columns=['ip_address', 'no_of_packets', \"no_of_bytes\", \"asn\", \"bgp_range\", \"asn_owner\", \"country_code\", \"country\", \"continent\", \"city\", \"location\"])\n",
    "dns_red_ip_df.set_index('ip_address', inplace=True)\n",
    "dns_red_ip_df.sort_values(by=['no_of_packets'], ascending=[False], inplace=True)\n",
    "# print top 20\n",
    "dns_red_ip_df[:20]\n",
    "\n",
    "# string = \"\"\n",
    "# for ip, _ in cve_14847_ip_data.items():\n",
    "#     if len(string) != 0:\n",
    "#         string += \" or \"\n",
    "#     string += \"ip.addr == \"\n",
    "#     string += ip\n",
    "# print(\"Wireshark filter:\")\n",
    "# # print(string)\n",
    "# print(cve_14847_traffic)\n",
    "# count = 0\n",
    "# for key, value in cve_14847_traffic.items():\n",
    "#     count += value\n",
    "# print(\"Total number of: {}  pakcets\".format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dns_red_asn_df = dns_red_ip_df.copy()\n",
    "dns_red_asn_df.reset_index(level=0, inplace=True)\n",
    "dns_red_asn_df = dns_red_asn_df.groupby(['asn', 'asn_owner']).agg({'no_of_packets': 'sum', 'no_of_bytes': 'sum', 'ip_address': 'nunique', 'bgp_range': 'unique'})\n",
    "dns_red_asn_df.sort_values(by=['no_of_packets'], ascending=[False], inplace=True)\n",
    "dns_red_asn_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mirai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dns_red_ip_data = {}\n",
    "dns_red_traffic = {}\n",
    "count = 0\n",
    "for (pkt_data, pkt_metadata,) in RawPcapReader(file_name_mirai):\n",
    "    ether_pkt = Ether(pkt_data)\n",
    "    ip_pkt = ether_pkt[IP]\n",
    "    tcp_pkt = ip_pkt[TCP]\n",
    "    \n",
    "    # skip packets in the direction server -> client\n",
    "    src = str(ip_pkt.src)\n",
    "    if src == real_server_ip:\n",
    "        continue\n",
    "    \n",
    "    count +=1\n",
    "    \n",
    "    # packet size\n",
    "    pkt_len = len(pkt_data)\n",
    "    \n",
    "    if src not in dns_red_ip_data:\n",
    "        asn, bgp_range = asndb.lookup(src)\n",
    "        asn = \"AS\" + str(asn)\n",
    "        asn_owner = aslist_dict[asn] if asn in aslist_dict else \"\"\n",
    "        ip_dict = ip_reader.get(src)\n",
    "        if ip_dict:\n",
    "            country_code = ip_dict['country']['iso_code'] if 'country' in ip_dict.keys() else \"N/A\"\n",
    "            country = ip_dict['country']['names']['en'] if 'country' in ip_dict.keys() else \"N/A\"\n",
    "            continent = ip_dict['continent']['code'] if 'continent'in ip_dict.keys() else \"N/A\"\n",
    "            city = ip_dict['city']['names']['en'] if 'city' in ip_dict.keys() else \"N/A\"\n",
    "            lat = ip_dict['location']['latitude'] if 'location' in ip_dict.keys() else \"N/A\"\n",
    "            long = ip_dict['location']['longitude'] if 'location' in ip_dict.keys() else \"N/A\"\n",
    "            lat_long = (lat, long)\n",
    "        else:\n",
    "            country_code = \"\"\n",
    "            country = \"\"\n",
    "            continent = \"\"\n",
    "            city = \"\"\n",
    "            lat_long = \"\"\n",
    "        dns_red_ip_data[src] = [1, pkt_len, asn, bgp_range, asn_owner, country_code, country, continent, city, lat_long]\n",
    "    else:\n",
    "        old_dns_red_ip_data = dns_red_ip_data[src]\n",
    "        new_dns_red_ip_data = deepcopy(old_dns_red_ip_data)\n",
    "        new_dns_red_ip_data[0] += 1\n",
    "        new_dns_red_ip_data[1] += pkt_len\n",
    "        dns_red_ip_data[src] = new_dns_red_ip_data\n",
    "        \n",
    "    # generate timestamp\n",
    "    pkt_sec = 1 # not available here\n",
    "    \n",
    "    # set timestamp seconds of first packet and last packet\n",
    "    if count == 1:\n",
    "        first_pkt_sec = pkt_timestamp // pkt_timestamp_resol\n",
    "    last_pkt_sec = pkt_timestamp // pkt_timestamp_resol\n",
    "    \n",
    "    # create 10 minute timeframe timestamp and set time at middle of time frame\n",
    "    time_period_middle_sec = pkt_sec - (pkt_sec - first_pkt_sec) % timestamp_period + timestamp_period // 2\n",
    "        \n",
    "    # generate continent time data dictionary\n",
    "    if str(time_period_middle_sec) not in dns_red_traffic:\n",
    "        dns_red_traffic[str(time_period_middle_sec)] = pkt_len\n",
    "    else:\n",
    "        dns_red_traffic[str(time_period_middle_sec)] += pkt_len        \n",
    "\n",
    "print(\"There are {} unique ip addresses!\".format(len(dns_red_ip_data)))\n",
    "\n",
    "dns_red_ip_res = []\n",
    "for ip, info in dns_red_ip_data.items():\n",
    "    res = [ip]\n",
    "    res.extend(info)\n",
    "    dns_red_ip_res.append(res)\n",
    "dns_red_ip_df = pd.DataFrame(dns_red_ip_res, columns=['ip_address', 'no_of_packets', \"no_of_bytes\", \"asn\", \"bgp_range\", \"asn_owner\", \"country_code\", \"country\", \"continent\", \"city\", \"location\"])\n",
    "dns_red_ip_df.set_index('ip_address', inplace=True)\n",
    "dns_red_ip_df.sort_values(by=['no_of_packets'], ascending=[False], inplace=True)\n",
    "# print top 20\n",
    "dns_red_ip_df[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dns_red_asn_df = dns_red_ip_df.copy()\n",
    "dns_red_asn_df.reset_index(level=0, inplace=True)\n",
    "dns_red_asn_df = dns_red_asn_df.groupby(['asn', 'asn_owner']).agg({'no_of_packets': 'sum', 'no_of_bytes': 'sum', 'ip_address': 'nunique', 'bgp_range': 'unique'})\n",
    "dns_red_asn_df.sort_values(by=['ip_address'], ascending=[False], inplace=True)\n",
    "dns_red_asn_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (Module 12 - Research Project)",
   "language": "python",
   "name": "pycharm-b14f5cc2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
